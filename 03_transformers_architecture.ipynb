{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "137042fe",
   "metadata": {},
   "source": [
    "![Python](https://img.shields.io/badge/python-3.9-blue)\n",
    "![Status: Pending Migration](https://img.shields.io/badge/status-pending%20migration-orange)\n",
    "\n",
    "<a id=\"table-of-contents\"></a>\n",
    "# 📖 Transformers: The Architecture That Changed NLP\n",
    "\n",
    "- [🧠 Why Transformers?](#why-transformers)\n",
    "  - [🔄 Limits of RNNs and CNNs for Sequential Data](#limits-of-rnns-cnns)\n",
    "  - [🔗 Need for Long-Range Dependencies](#long-range-dependencies)\n",
    "  - [⏱️ Parallelism and Efficiency](#parallelism-efficiency)\n",
    "- [🏗️ Core Building Blocks](#core-building-blocks)\n",
    "  - [📦 Embeddings](#embeddings)\n",
    "  - [🎯 Positional Encoding](#positional-encoding)\n",
    "  - [🧮 Self-Attention Mechanism](#self-attention)\n",
    "  - [🧠 Multi-head Attention](#multihead-attention)\n",
    "  - [🧱 Feedforward Layers](#feedforward-layers)\n",
    "  - [🔁 Layer Norm, Skip Connections](#layer-norm)\n",
    "- [🔬 The Transformer Block](#transformer-block)\n",
    "  - [🔁 Encoder Block (Structure + Flow)](#encoder-block)\n",
    "  - [🔁 Decoder Block (Structure + Flow)](#decoder-block)\n",
    "  - [🔄 Masking in Attention](#masking)\n",
    "  - [📶 Stack of N Layers](#stack-of-layers)\n",
    "- [🔢 Attention Mechanism in Depth](#attention-in-depth)\n",
    "  - [🧠 Attention as Weighted Lookup](#attention-weighted-lookup)\n",
    "  - [📏 Query, Key, Value Vectors](#qkv-vectors)\n",
    "  - [📊 Dot-Product Attention Calculation](#dot-product-attention)\n",
    "  - [⚙️ Softmax + Scaling](#softmax-scaling)\n",
    "- [🧰 Training a Transformer Model](#training-transformer)\n",
    "  - [📊 Example: Sequence Classification or Translation](#sequence-task-example)\n",
    "  - [💡 Tokenization (WordPiece/BPE) Basics](#tokenization)\n",
    "  - [🧮 Input-Output Pipeline](#input-output-pipeline)\n",
    "- [📚 Transformers Beyond Text](#beyond-text)\n",
    "  - [🧠 Use in Vision (ViT)](#transformers-vision)\n",
    "  - [🧪 Time Series and Tabular Data](#transformers-time-series)\n",
    "  - [🧬 Multimodal Transformers](#multimodal-transformers)\n",
    "- [🧭 From Transformers to LLMs](#to-llms)\n",
    "  - [🌍 Evolution: Transformer → GPT/BERT → LLMs](#evolution)\n",
    "  - [📈 Scaling Laws (Depth, Width, Data)](#scaling-laws)\n",
    "  - [🔍 Pretraining Objectives: Causal vs. Masked](#pretraining-objectives)\n",
    "- [🔚 Closing Notes](#closing-notes)\n",
    "  - [⚠️ Conceptual Pitfalls](#pitfalls)\n",
    "  - [🔍 Visual Explainers and Demos to Explore](#visual-explainers)\n",
    "  - [🚀 Next Up: Large Language Models (04)](#next-llms)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a44c608",
   "metadata": {},
   "source": [
    "<a id=\"why-transformers\"></a>\n",
    "# 🧠 Why Transformers?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97bdb92",
   "metadata": {},
   "source": [
    "<a id=\"limits-of-rnns-cnns\"></a>\n",
    "#### 🔄 Limits of RNNs and CNNs for Sequential Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9944ad7",
   "metadata": {},
   "source": [
    "<a id=\"long-range-dependencies\"></a>\n",
    "#### 🔗 Need for Long-Range Dependencies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df012b9",
   "metadata": {},
   "source": [
    "<a id=\"parallelism-efficiency\"></a>\n",
    "#### ⏱️ Parallelism and Efficiency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dd8851",
   "metadata": {},
   "source": [
    "[Back to the top](#table-of-contents)\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72666e3",
   "metadata": {},
   "source": [
    "<a id=\"core-building-blocks\"></a>\n",
    "# 🏗️ Core Building Blocks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e13df38",
   "metadata": {},
   "source": [
    "<a id=\"embeddings\"></a>\n",
    "#### 📦 Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d718593a",
   "metadata": {},
   "source": [
    "<a id=\"positional-encoding\"></a>\n",
    "#### 🎯 Positional Encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778d521e",
   "metadata": {},
   "source": [
    "<a id=\"self-attention\"></a>\n",
    "#### 🧮 Self-Attention Mechanism\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e05be96",
   "metadata": {},
   "source": [
    "<a id=\"multihead-attention\"></a>\n",
    "#### 🧠 Multi-head Attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebb9645",
   "metadata": {},
   "source": [
    "<a id=\"feedforward-layers\"></a>\n",
    "#### 🧱 Feedforward Layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0b1263",
   "metadata": {},
   "source": [
    "<a id=\"layer-norm\"></a>\n",
    "#### 🔁 Layer Norm, Skip Connections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe7b4d4",
   "metadata": {},
   "source": [
    "[Back to the top](#table-of-contents)\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b9878e",
   "metadata": {},
   "source": [
    "<a id=\"transformer-block\"></a>\n",
    "# 🔬 The Transformer Block\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1382dcaa",
   "metadata": {},
   "source": [
    "<a id=\"encoder-block\"></a>\n",
    "#### 🔁 Encoder Block (Structure + Flow)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742f01e1",
   "metadata": {},
   "source": [
    "<a id=\"decoder-block\"></a>\n",
    "#### 🔁 Decoder Block (Structure + Flow)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7bb318",
   "metadata": {},
   "source": [
    "<a id=\"masking\"></a>\n",
    "#### 🔄 Masking in Attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bece8b18",
   "metadata": {},
   "source": [
    "<a id=\"stack-of-layers\"></a>\n",
    "#### 📶 Stack of N Layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c38f68",
   "metadata": {},
   "source": [
    "[Back to the top](#table-of-contents)\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be36224f",
   "metadata": {},
   "source": [
    "<a id=\"attention-in-depth\"></a>\n",
    "# 🔢 Attention Mechanism in Depth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ac0b56",
   "metadata": {},
   "source": [
    "<a id=\"attention-weighted-lookup\"></a>\n",
    "#### 🧠 Attention as Weighted Lookup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b44ba84",
   "metadata": {},
   "source": [
    "<a id=\"qkv-vectors\"></a>\n",
    "#### 📏 Query, Key, Value Vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b95f82",
   "metadata": {},
   "source": [
    "<a id=\"dot-product-attention\"></a>\n",
    "#### 📊 Dot-Product Attention Calculation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5444ab5f",
   "metadata": {},
   "source": [
    "<a id=\"softmax-scaling\"></a>\n",
    "#### ⚙️ Softmax + Scaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0d45ec",
   "metadata": {},
   "source": [
    "[Back to the top](#table-of-contents)\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8339ad0",
   "metadata": {},
   "source": [
    "<a id=\"training-transformer\"></a>\n",
    "# 🧰 Training a Transformer Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5ae73d",
   "metadata": {},
   "source": [
    "<a id=\"sequence-task-example\"></a>\n",
    "#### 📊 Example: Sequence Classification or Translation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8a0ab2",
   "metadata": {},
   "source": [
    "<a id=\"tokenization\"></a>\n",
    "#### 💡 Tokenization (WordPiece/BPE) Basics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0269315a",
   "metadata": {},
   "source": [
    "<a id=\"input-output-pipeline\"></a>\n",
    "#### 🧮 Input-Output Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a893a802",
   "metadata": {},
   "source": [
    "[Back to the top](#table-of-contents)\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29287f00",
   "metadata": {},
   "source": [
    "<a id=\"beyond-text\"></a>\n",
    "# 📚 Transformers Beyond Text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd587708",
   "metadata": {},
   "source": [
    "<a id=\"transformers-vision\"></a>\n",
    "#### 🧠 Use in Vision (ViT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10366f94",
   "metadata": {},
   "source": [
    "<a id=\"transformers-time-series\"></a>\n",
    "#### 🧪 Time Series and Tabular Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f63ac2",
   "metadata": {},
   "source": [
    "<a id=\"multimodal-transformers\"></a>\n",
    "#### 🧬 Multimodal Transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a804535",
   "metadata": {},
   "source": [
    "[Back to the top](#table-of-contents)\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046a3c9c",
   "metadata": {},
   "source": [
    "<a id=\"to-llms\"></a>\n",
    "# 🧭 From Transformers to LLMs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a87c2d8",
   "metadata": {},
   "source": [
    "<a id=\"evolution\"></a>\n",
    "#### 🌍 Evolution: Transformer → GPT/BERT → LLMs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7743d7e5",
   "metadata": {},
   "source": [
    "<a id=\"scaling-laws\"></a>\n",
    "#### 📈 Scaling Laws (Depth, Width, Data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae48e31",
   "metadata": {},
   "source": [
    "<a id=\"pretraining-objectives\"></a>\n",
    "#### 🔍 Pretraining Objectives: Causal vs. Masked\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b00966a",
   "metadata": {},
   "source": [
    "[Back to the top](#table-of-contents)\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e372ce5",
   "metadata": {},
   "source": [
    "<a id=\"closing-notes\"></a>\n",
    "# 🔚 Closing Notes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee653916",
   "metadata": {},
   "source": [
    "<a id=\"pitfalls\"></a>\n",
    "#### ⚠️ Conceptual Pitfalls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516c347b",
   "metadata": {},
   "source": [
    "<a id=\"visual-explainers\"></a>\n",
    "#### 🔍 Visual Explainers and Demos to Explore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e805851a",
   "metadata": {},
   "source": [
    "<a id=\"next-llms\"></a>\n",
    "#### 🚀 Next Up: Large Language Models (04)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5ced4d",
   "metadata": {},
   "source": [
    "[Back to the top](#table-of-contents)\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
